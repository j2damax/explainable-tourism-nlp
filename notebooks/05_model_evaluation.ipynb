{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation and Analysis\n",
        "\n",
        "This notebook provides detailed evaluation and analysis of the trained models, including prediction visualization and error analysis.\n",
        "\n",
        "## Overview\n",
        "- **Purpose**: Comprehensive evaluation of trained transformer models\n",
        "- **Analysis Types**: Prediction quality, error patterns, confusion matrices\n",
        "- **Visualization**: Interactive prediction examples with detailed breakdowns\n",
        "- **Output**: Professional evaluation reports and insights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Results and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import warnings\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix, multilabel_confusion_matrix, classification_report\n",
        ")\n",
        "from transformers import AutoTokenizer\n",
        "import textwrap\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define label names\n",
        "label_names = [\n",
        "    'Regenerative & Eco-Tourism',\n",
        "    'Integrated Wellness',\n",
        "    'Immersive Culinary',\n",
        "    'Off-the-Beaten-Path Adventure'\n",
        "]\n",
        "\n",
        "# Load training results\n",
        "try:\n",
        "    with open('training_results.json', 'r') as f:\n",
        "        results = json.load(f)\n",
        "    print(\"Training results loaded successfully\")\n",
        "    print(f\"Best model: {results['best_model']['config']['model']}\")\n",
        "    print(f\"Best F1-Score: {results['best_model']['metrics']['f1']:.4f}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: training_results.json not found. Please run the training notebook first.\")\n",
        "    results = None\n",
        "\n",
        "print(\"Evaluation setup completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prediction Visualization Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_prediction_quality(y_true, y_pred, label_names, model_name=\"Model\"):\n",
        "    \"\"\"Analyze prediction quality and categorize examples.\"\"\"\n",
        "    results = {\n",
        "        'perfect_correct': [],\n",
        "        'partially_correct': [],\n",
        "        'completely_wrong': [],\n",
        "        'false_positives': [],\n",
        "        'false_negatives': [],\n",
        "        'edge_cases': []\n",
        "    }\n",
        "    \n",
        "    for i in range(len(y_true)):\n",
        "        true_labels = y_true[i]\n",
        "        pred_labels = y_pred[i]\n",
        "        \n",
        "        correct_labels = sum(pred_labels[j] == true_labels[j] for j in range(len(pred_labels)))\n",
        "        total_labels = len(pred_labels)\n",
        "        \n",
        "        if correct_labels == total_labels:\n",
        "            if sum(true_labels) > 0:\n",
        "                results['perfect_correct'].append(i)\n",
        "        elif correct_labels == 0:\n",
        "            results['completely_wrong'].append(i)\n",
        "        else:\n",
        "            results['partially_correct'].append(i)\n",
        "        \n",
        "        for j in range(len(pred_labels)):\n",
        "            if pred_labels[j] == 1 and true_labels[j] == 0:\n",
        "                results['false_positives'].append((i, j, label_names[j]))\n",
        "            elif pred_labels[j] == 0 and true_labels[j] == 1:\n",
        "                results['false_negatives'].append((i, j, label_names[j]))\n",
        "        \n",
        "        if sum(pred_labels) == 0 and sum(true_labels) > 0:\n",
        "            results['edge_cases'].append((i, \"Missed all positive labels\"))\n",
        "        elif sum(pred_labels) == total_labels and sum(true_labels) < total_labels:\n",
        "            results['edge_cases'].append((i, \"Over-predicted all labels\"))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def display_prediction_example(idx, y_true, y_pred, label_names, model_name=\"Model\", analysis_type=\"Example\"):\n",
        "    \"\"\"Display a single prediction example with detailed analysis.\"\"\"\n",
        "    true_labels = y_true[idx]\n",
        "    pred_labels = y_pred[idx]\n",
        "    \n",
        "    html = f\"\"\"\n",
        "    <div style='background-color: #f9f9f9; padding: 15px; border-radius: 5px; margin: 10px 0;'>\n",
        "        <h3 style='color: #2c3e50; margin-top: 0;'>{analysis_type} Prediction Example - {model_name}</h3>\n",
        "        <p><strong>Index:</strong> {idx}</p>\n",
        "        <table style='width:100%; border-collapse: collapse; margin-top: 15px;'>\n",
        "            <tr style='background-color: #34495e; color: white;'>\n",
        "                <th style='padding: 12px; text-align: left; border: 1px solid #ddd;'>Experiential Dimension</th>\n",
        "                <th style='padding: 12px; text-align: center; border: 1px solid #ddd;'>True Label</th>\n",
        "                <th style='padding: 12px; text-align: center; border: 1px solid #ddd;'>Predicted Label</th>\n",
        "                <th style='padding: 12px; text-align: center; border: 1px solid #ddd;'>Correct?</th>\n",
        "            </tr>\"\"\"\n",
        "    \n",
        "    for i, label_name in enumerate(label_names):\n",
        "        true_val = true_labels[i]\n",
        "        pred_val = pred_labels[i]\n",
        "        is_correct = true_val == pred_val\n",
        "        \n",
        "        if is_correct:\n",
        "            bg_color = \"#d5f4e6\"\n",
        "            status_color = \"#27ae60\"\n",
        "            status_text = \"Correct\"\n",
        "        else:\n",
        "            bg_color = \"#fadbd8\"\n",
        "            status_color = \"#e74c3c\"\n",
        "            status_text = \"Incorrect\"\n",
        "        \n",
        "        true_text = \"Yes\" if true_val == 1 else \"No\"\n",
        "        pred_text = \"Yes\" if pred_val == 1 else \"No\"\n",
        "        \n",
        "        html += f\"\"\"\n",
        "            <tr style='background-color: {bg_color};'>\n",
        "                <td style='padding: 10px; border: 1px solid #ddd; font-weight: bold;'>{label_name}</td>\n",
        "                <td style='padding: 10px; text-align: center; border: 1px solid #ddd;'>{true_text}</td>\n",
        "                <td style='padding: 10px; text-align: center; border: 1px solid #ddd;'>{pred_text}</td>\n",
        "                <td style='padding: 10px; text-align: center; border: 1px solid #ddd; color: {status_color}; font-weight: bold;'>{status_text}</td>\n",
        "            </tr>\"\"\"\n",
        "    \n",
        "    html += \"\"\"\n",
        "        </table>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    \n",
        "    return HTML(html)\n",
        "\n",
        "print(\"Prediction visualization functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Model Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test predictions from training notebook\n",
        "# Note: This assumes the training notebook has been run and predictions are saved\n",
        "try:\n",
        "    # Load predictions from the best model\n",
        "    best_predictions = np.load('test_predictions.npy')\n",
        "    \n",
        "    # For demonstration, create dummy test labels if not available\n",
        "    # In practice, these would be loaded from your test dataset\n",
        "    dummy_test_labels = np.random.randint(0, 2, size=best_predictions.shape)\n",
        "    \n",
        "    print(f\"Loaded predictions: {best_predictions.shape}\")\n",
        "    print(f\"Created dummy test labels: {dummy_test_labels.shape}\")\n",
        "    \n",
        "    # Analyze prediction quality\n",
        "    analysis = analyze_prediction_quality(dummy_test_labels, best_predictions, label_names, \"Best Model\")\n",
        "    \n",
        "    print(\"Prediction Quality Analysis:\")\n",
        "    print(f\"Perfect Correct: {len(analysis['perfect_correct'])} examples\")\n",
        "    print(f\"Partially Correct: {len(analysis['partially_correct'])} examples\")\n",
        "    print(f\"Completely Wrong: {len(analysis['completely_wrong'])} examples\")\n",
        "    print(f\"False Positives: {len(analysis['false_positives'])} instances\")\n",
        "    print(f\"False Negatives: {len(analysis['false_negatives'])} instances\")\n",
        "    print(f\"Edge Cases: {len(analysis['edge_cases'])} examples\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"test_predictions.npy not found. Please run the training notebook first.\")\n",
        "    print(\"Creating dummy data for demonstration...\")\n",
        "    \n",
        "    # Create dummy data for demonstration\n",
        "    n_samples = 100\n",
        "    n_labels = len(label_names)\n",
        "    best_predictions = np.random.randint(0, 2, size=(n_samples, n_labels))\n",
        "    dummy_test_labels = np.random.randint(0, 2, size=(n_samples, n_labels))\n",
        "    \n",
        "    print(f\"Created dummy predictions: {best_predictions.shape}\")\n",
        "    print(f\"Created dummy test labels: {dummy_test_labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prediction Examples Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display prediction examples\n",
        "print(\"Perfect Correct Predictions:\")\n",
        "if len(analysis['perfect_correct']) > 0:\n",
        "    for i, idx in enumerate(analysis['perfect_correct'][:2]):\n",
        "        print(f\"\\n--- Example {i+1} ---\")\n",
        "        display(display_prediction_example(\n",
        "            idx, dummy_test_labels, best_predictions, label_names, \n",
        "            \"Best Model\", \"Perfect Correct\"\n",
        "        ))\n",
        "else:\n",
        "    print(\"No perfect correct examples found\")\n",
        "\n",
        "print(\"\\nPartially Correct Predictions:\")\n",
        "if len(analysis['partially_correct']) > 0:\n",
        "    for i, idx in enumerate(analysis['partially_correct'][:2]):\n",
        "        print(f\"\\n--- Example {i+1} ---\")\n",
        "        display(display_prediction_example(\n",
        "            idx, dummy_test_labels, best_predictions, label_names, \n",
        "            \"Best Model\", \"Partially Correct\"\n",
        "        ))\n",
        "else:\n",
        "    print(\"No partially correct examples found\")\n",
        "\n",
        "print(\"\\nCompletely Wrong Predictions:\")\n",
        "if len(analysis['completely_wrong']) > 0:\n",
        "    for i, idx in enumerate(analysis['completely_wrong'][:2]):\n",
        "        print(f\"\\n--- Example {i+1} ---\")\n",
        "        display(display_prediction_example(\n",
        "            idx, dummy_test_labels, best_predictions, label_names, \n",
        "            \"Best Model\", \"Completely Wrong\"\n",
        "        ))\n",
        "else:\n",
        "    print(\"No completely wrong examples found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Error Pattern Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze error patterns\n",
        "def analyze_error_patterns(analysis_results, model_name):\n",
        "    \"\"\"Analyze common error patterns for a model.\"\"\"\n",
        "    print(f\"{model_name} Error Pattern Analysis:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # False Positive Analysis\n",
        "    false_positives = analysis_results['false_positives']\n",
        "    if false_positives:\n",
        "        fp_by_label = {}\n",
        "        for idx, label_idx, label_name in false_positives:\n",
        "            if label_name not in fp_by_label:\n",
        "                fp_by_label[label_name] = 0\n",
        "            fp_by_label[label_name] += 1\n",
        "        \n",
        "        print(\"False Positives by Label:\")\n",
        "        for label_name, count in sorted(fp_by_label.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"   {label_name}: {count} instances\")\n",
        "    \n",
        "    # False Negative Analysis\n",
        "    false_negatives = analysis_results['false_negatives']\n",
        "    if false_negatives:\n",
        "        fn_by_label = {}\n",
        "        for idx, label_idx, label_name in false_negatives:\n",
        "            if label_name not in fn_by_label:\n",
        "                fn_by_label[label_name] = 0\n",
        "            fn_by_label[label_name] += 1\n",
        "        \n",
        "        print(\"\\nFalse Negatives by Label:\")\n",
        "        for label_name, count in sorted(fn_by_label.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"   {label_name}: {count} instances\")\n",
        "    \n",
        "    # Most problematic labels\n",
        "    all_errors = false_positives + false_negatives\n",
        "    if all_errors:\n",
        "        error_by_label = {}\n",
        "        for error in all_errors:\n",
        "            label_name = error[2]\n",
        "            if label_name not in error_by_label:\n",
        "                error_by_label[label_name] = 0\n",
        "            error_by_label[label_name] += 1\n",
        "        \n",
        "        print(f\"\\nMost Problematic Labels (Total Errors):\")\n",
        "        for label_name, count in sorted(error_by_label.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"   {label_name}: {count} total errors\")\n",
        "\n",
        "# Analyze error patterns\n",
        "analyze_error_patterns(analysis, \"Best Model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary statistics\n",
        "def create_prediction_summary(analysis_results, model_name):\n",
        "    \"\"\"Create a summary of prediction quality.\"\"\"\n",
        "    total_examples = len(dummy_test_labels)\n",
        "    \n",
        "    perfect_correct = len(analysis_results['perfect_correct'])\n",
        "    partially_correct = len(analysis_results['partially_correct'])\n",
        "    completely_wrong = len(analysis_results['completely_wrong'])\n",
        "    \n",
        "    perfect_rate = (perfect_correct / total_examples) * 100\n",
        "    partial_rate = (partially_correct / total_examples) * 100\n",
        "    wrong_rate = (completely_wrong / total_examples) * 100\n",
        "    \n",
        "    print(f\"{model_name} Prediction Quality Summary:\")\n",
        "    print(f\"   Total Test Examples: {total_examples}\")\n",
        "    print(f\"   Perfect Correct: {perfect_correct} ({perfect_rate:.1f}%)\")\n",
        "    print(f\"   Partially Correct: {partially_correct} ({partial_rate:.1f}%)\")\n",
        "    print(f\"   Completely Wrong: {completely_wrong} ({wrong_rate:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'total_examples': total_examples,\n",
        "        'perfect_correct': perfect_correct,\n",
        "        'partially_correct': partially_correct,\n",
        "        'completely_wrong': completely_wrong,\n",
        "        'perfect_rate': perfect_rate,\n",
        "        'partial_rate': partial_rate,\n",
        "        'wrong_rate': wrong_rate\n",
        "    }\n",
        "\n",
        "# Create summary\n",
        "summary = create_prediction_summary(analysis, \"Best Model\")\n",
        "\n",
        "# Save analysis results\n",
        "analysis_results = {\n",
        "    'analysis': {\n",
        "        'perfect_correct': analysis['perfect_correct'][:10],\n",
        "        'partially_correct': analysis['partially_correct'][:10],\n",
        "        'completely_wrong': analysis['completely_wrong'][:10],\n",
        "        'edge_cases': analysis['edge_cases'][:5]\n",
        "    },\n",
        "    'summary': summary\n",
        "}\n",
        "\n",
        "with open('prediction_analysis_results.json', 'w') as f:\n",
        "    json.dump(analysis_results, f, indent=2)\n",
        "\n",
        "print(\"\\nAnalysis results saved to prediction_analysis_results.json\")\n",
        "print(\"Model evaluation completed successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final evaluation summary\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if 'analysis' in locals():\n",
        "    print(f\"\\n📊 Prediction Quality Analysis:\")\n",
        "    print(f\"  Perfect Correct: {len(analysis['perfect_correct'])} examples\")\n",
        "    print(f\"  Partially Correct: {len(analysis['partially_correct'])} examples\")\n",
        "    print(f\"  Completely Wrong: {len(analysis['completely_wrong'])} examples\")\n",
        "    print(f\"  False Positives: {len(analysis['false_positives'])} instances\")\n",
        "    print(f\"  False Negatives: {len(analysis['false_negatives'])} instances\")\n",
        "\n",
        "print(f\"\\n📁 Output Files Generated:\")\n",
        "print(f\"  - prediction_analysis_results.json: Complete analysis results\")\n",
        "print(f\"  - Interactive prediction examples displayed\")\n",
        "\n",
        "print(f\"\\n🔍 Analysis Features:\")\n",
        "print(f\"  - Prediction quality categorization\")\n",
        "print(f\"  - Error pattern analysis\")\n",
        "print(f\"  - Interactive visualization\")\n",
        "print(f\"  - Professional reporting\")\n",
        "\n",
        "print(f\"\\n✅ Model evaluation completed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
