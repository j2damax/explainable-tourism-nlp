{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "66712107",
      "metadata": {
        "id": "66712107"
      },
      "source": [
        "# Model Training and Evaluation\n",
        "\n",
        "This notebook covers the definition, training, and evaluation of a transformer-based multi-label text classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a2b576",
      "metadata": {
        "id": "87a2b576"
      },
      "source": [
        "## 1. Load Prepared Datasets and Libraries\n",
        "\n",
        "Load the PyTorch datasets and import required libraries for model training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70d49bc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160ef9c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install databricks-sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ec1b6155",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec1b6155",
        "outputId": "5856f5ff-757f-4841-ecd2-9df4be3ab0cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0175ef35",
      "metadata": {
        "id": "0175ef35"
      },
      "outputs": [],
      "source": [
        "# Import libraries and load datasets\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "from transformers import (\n",
        "    BertForSequenceClassification, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7fc08f98",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables from .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "25366548",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25366548",
        "outputId": "61d68e71-b748-4f32-b9bf-9c0efe244ff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/3866477269740015', creation_time=1758424394648, experiment_id='3866477269740015', last_update_time=1758427431013, lifecycle_stage='active', name='/Users/j2damax@gmail.com/serendip-travel-review-classifier-experiments', tags={'mlflow.experiment.sourceName': '/Users/j2damax@gmail.com/serendip-travel-review-classifier-experiments',\n",
              " 'mlflow.experimentKind': 'genai_development',\n",
              " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
              " 'mlflow.ownerEmail': 'j2damax@gmail.com',\n",
              " 'mlflow.ownerId': '5804221812504751'}>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set Databricks credentials from environment variables (do not hardcode in code!)\n",
        "import os\n",
        "import mlflow\n",
        "DATABRICKS_HOST = os.getenv(\"DATABRICKS_HOST\")\n",
        "DATABRICKS_TOKEN = os.getenv(\"DATABRICKS_TOKEN\")\n",
        "if not DATABRICKS_HOST or not DATABRICKS_TOKEN:\n",
        "    raise ValueError(\"DATABRICKS_HOST and DATABRICKS_TOKEN must be set in your environment.\\n\\nIn Colab, use:\\n%env DATABRICKS_HOST=https://<your-databricks-instance>\\n%env DATABRICKS_TOKEN=<your-token>\\n\\nLocally, set them in your shell or .env file.\")\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "mlflow.set_experiment(\"/Users/j2damax@gmail.com/serendip-travel-review-classifier-experiments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1000a64a",
      "metadata": {
        "id": "1000a64a"
      },
      "outputs": [],
      "source": [
        "# Define ReviewsDataset class (must match the one used in 03_modeling.ipynb)\n",
        "from torch.utils.data import Dataset\n",
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c29a52de",
      "metadata": {
        "id": "c29a52de"
      },
      "outputs": [],
      "source": [
        "#train_data = torch.load('../data/processed/train_dataset.pt', weights_only=False)\n",
        "#test_data = torch.load('../data/processed/test_dataset.pt', weights_only=False)\n",
        "\n",
        "# Load datasets from Google Drive\n",
        "train_data = torch.load('/content/drive/MyDrive/SerendipTravel/data/processed/train_dataset.pt', weights_only=False)\n",
        "test_data = torch.load('/content/drive/MyDrive/SerendipTravel/data/processed/test_dataset.pt', weights_only=False)\n",
        "\n",
        "# Reuse your ReviewsDataset class definition here\n",
        "train_dataset = ReviewsDataset(train_data['encodings'], train_data['labels'])\n",
        "test_dataset = ReviewsDataset(test_data['encodings'], test_data['labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "822f1aaf",
      "metadata": {
        "id": "822f1aaf"
      },
      "source": [
        "## 2. Model Definition and Configuration\n",
        "\n",
        "Define the transformer model for multi-label classification and set up optimizer, loss, and training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1192ccd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1192ccd1",
        "outputId": "a68cf716-45d0-4014-8f20-84c747e241c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of labels: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Model definition and configuration\n",
        "num_labels = train_data['labels'].shape[1]\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=num_labels,\n",
        "    problem_type='multi_label_classification'\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 16\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Loss function for multi-label\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fcdac3e",
      "metadata": {
        "id": "5fcdac3e"
      },
      "source": [
        "## 3. Model Training and Evaluation\n",
        "\n",
        "Train the model and evaluate its performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0aceb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd0aceb3",
        "outputId": "a8a8f88d-c508-4667-c5bd-348cb8d9fa9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Training loss: 0.0573\n",
            "Epoch 2/3 - Training loss: 0.0573\n",
            "Epoch 3/3 - Training loss: 0.0578\n",
            "Test accuracy: 0.9245049504950495\n",
            "Test macro F1: 0.9298934093407507\n",
            "🏃 View run bert-multilabel-baseline at: https://dbc-cfeb31c8-2841.cloud.databricks.com/ml/experiments/3866477269740015/runs/5728587524314f69b56434d8d58e9490\n",
            "🧪 View experiment at: https://dbc-cfeb31c8-2841.cloud.databricks.com/ml/experiments/3866477269740015\n"
          ]
        }
      ],
      "source": [
        "# Training and evaluation loop with MLflow logging\n",
        "\n",
        "with mlflow.start_run(run_name=\"bert-multilabel-baseline\"):\n",
        "    # Log hyperparameters\n",
        "    mlflow.log_param(\"epochs\", epochs)\n",
        "    mlflow.log_param(\"batch_size\", batch_size)\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"model_name\", \"bert-base-uncased\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training loss: {avg_train_loss:.4f}\")\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].cpu().numpy()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits.cpu().numpy()\n",
        "            preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    import numpy as np\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    from sklearn.metrics import accuracy_score, f1_score\n",
        "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    test_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    print(\"Test accuracy:\", test_accuracy)\n",
        "    print(\"Test macro F1:\", test_macro_f1)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "    mlflow.log_metric(\"test_macro_f1\", test_macro_f1)\n",
        "\n",
        "    # Prepare a sample input and output\n",
        "    example_batch = next(iter(test_loader))\n",
        "    inputs = {\n",
        "        \"input_ids\": example_batch[\"input_ids\"][:1].cpu().numpy(),\n",
        "        \"attention_mask\": example_batch[\"attention_mask\"][:1].cpu().numpy()\n",
        "    }\n",
        "    outputs = model(\n",
        "        input_ids=example_batch[\"input_ids\"][:1].to(device),\n",
        "        attention_mask=example_batch[\"attention_mask\"][:1].to(device)\n",
        "    ).logits.cpu().detach().numpy()\n",
        "\n",
        "    signature = infer_signature(inputs, outputs)\n",
        "\n",
        "    mlflow.pytorch.log_model(\n",
        "        model,\n",
        "        name=\"model\",\n",
        "        signature=signature,\n",
        "        pip_requirements=[\"torch==2.8.0+cu126\", \"torchvision==0.23.0+cu126\"]\n",
        "    )\n",
        "\n",
        "    np.save(\"predictions.npy\", all_preds)\n",
        "    mlflow.log_artifact(\"predictions.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b4a3c69",
      "metadata": {},
      "source": [
        "## 4. Model Training and Evaluation - Optimzed, resuable function\n",
        "\n",
        "Train the model and evaluate its performance on the test set by parsing different arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909a4b83",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(\n",
        "    model_name=\"bert-base-uncased\",\n",
        "    epochs=3,\n",
        "    batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    train_dataset=None,\n",
        "    test_dataset=None,\n",
        "    num_labels=None,\n",
        "    experiment_name=\"bert-multilabel-baseline\",\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train and evaluate a transformer model for multi-label classification.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Hugging Face model name (e.g., 'bert-base-uncased', 'roberta-base').\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training and evaluation.\n",
        "        learning_rate (float): Learning rate for optimizer.\n",
        "        train_dataset (Dataset): PyTorch Dataset for training.\n",
        "        test_dataset (Dataset): PyTorch Dataset for evaluation.\n",
        "        num_labels (int): Number of output labels.\n",
        "        experiment_name (str): Name for MLflow experiment run.\n",
        "        seed (int, optional): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        all_preds (np.ndarray): Predicted labels for test set.\n",
        "        all_labels (np.ndarray): True labels for test set.\n",
        "        model (nn.Module): Trained model.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    from torch.utils.data import DataLoader\n",
        "    from transformers import BertForSequenceClassification, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
        "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "    import mlflow\n",
        "    import mlflow.pytorch\n",
        "    import json\n",
        "\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Model selection\n",
        "    if model_name.startswith(\"roberta\"):\n",
        "        model_cls = RobertaForSequenceClassification\n",
        "    else:\n",
        "        model_cls = BertForSequenceClassification\n",
        "    model = model_cls.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        problem_type='multi_label_classification'\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    with mlflow.start_run(run_name=experiment_name):\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"batch_size\", batch_size)\n",
        "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "        mlflow.log_param(\"model_name\", model_name)\n",
        "        if seed is not None:\n",
        "            mlflow.log_param(\"seed\", seed)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_train_loss = total_loss / len(train_loader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Training loss: {avg_train_loss:.4f}\")\n",
        "            mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].cpu().numpy()\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits.cpu().numpy()\n",
        "                preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
        "                all_preds.append(preds)\n",
        "                all_labels.append(labels)\n",
        "\n",
        "        all_preds = np.vstack(all_preds)\n",
        "        all_labels = np.vstack(all_labels)\n",
        "\n",
        "        # Metrics\n",
        "        test_accuracy = accuracy_score(all_labels, all_preds)\n",
        "        test_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "        test_macro_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        test_macro_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        print(\"Test accuracy:\", test_accuracy)\n",
        "        print(\"Test macro F1:\", test_macro_f1)\n",
        "        print(\"Test macro precision:\", test_macro_precision)\n",
        "        print(\"Test macro recall:\", test_macro_recall)\n",
        "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "        mlflow.log_metric(\"test_macro_f1\", test_macro_f1)\n",
        "        mlflow.log_metric(\"test_macro_precision\", test_macro_precision)\n",
        "        mlflow.log_metric(\"test_macro_recall\", test_macro_recall)\n",
        "\n",
        "        # Per-label F1, precision, recall\n",
        "        per_label_f1 = f1_score(all_labels, all_preds, average=None)\n",
        "        per_label_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "        per_label_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "        for i, (f1, p, r) in enumerate(zip(per_label_f1, per_label_precision, per_label_recall)):\n",
        "            mlflow.log_metric(f\"f1_label_{i}\", f1)\n",
        "            mlflow.log_metric(f\"precision_label_{i}\", p)\n",
        "            mlflow.log_metric(f\"recall_label_{i}\", r)\n",
        "\n",
        "        # Save model and predictions\n",
        "        mlflow.pytorch.log_model(model, \"model\")\n",
        "        np.save(\"predictions.npy\", all_preds)\n",
        "        mlflow.log_artifact(\"predictions.npy\")\n",
        "        # Optionally, log classification report\n",
        "        report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "        with open(\"classification_report.json\", \"w\") as f:\n",
        "            json.dump(report, f)\n",
        "        mlflow.log_artifact(\"classification_report.json\")\n",
        "\n",
        "    return all_preds, all_labels, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f12c2f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter sweep (loop)\n",
        "results = []\n",
        "for lr in [2e-5, 1e-5]:\n",
        "    for bs in [8, 16]:\n",
        "        print(f\"\\nRunning with learning_rate={lr}, batch_size={bs}\")\n",
        "        preds, labels, mdl = train_and_evaluate(\n",
        "            model_name=\"bert-base-uncased\",\n",
        "            epochs=3,\n",
        "            batch_size=bs,\n",
        "            learning_rate=lr,\n",
        "            train_dataset=train_dataset,\n",
        "            test_dataset=test_dataset,\n",
        "            num_labels=num_labels,\n",
        "            experiment_name=f\"bert-lr{lr}-bs{bs}\"\n",
        "        )\n",
        "        results.append({\"lr\": lr, \"batch_size\": bs, \"preds\": preds, \"labels\": labels})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b4e3b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT run\n",
        "all_preds, all_labels, model = train_and_evaluate(\n",
        "    model_name=\"bert-base-uncased\",\n",
        "    epochs=3,\n",
        "    batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    train_dataset=train_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    num_labels=num_labels,\n",
        "    experiment_name=\"bert-baseline\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6e0ff6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# RoBERTa with different hyperparameters\n",
        "all_preds_roberta, all_labels_roberta, model_roberta = train_and_evaluate(\n",
        "    model_name=\"roberta-base\",\n",
        "    epochs=5,\n",
        "    batch_size=8,\n",
        "    learning_rate=1e-5,\n",
        "    train_dataset=train_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    num_labels=num_labels,\n",
        "    experiment_name=\"roberta-sweep\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.4)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
