{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hugging Face Model Deployment\n",
        "\n",
        "This notebook downloads the best performing model artifacts from MLflow and prepares them for deployment on Hugging Face Hub.\n",
        "\n",
        "## Overview\n",
        "- **Objective**: Download and prepare the best BERT model for Hugging Face deployment\n",
        "- **MLflow Run ID**: ff204ab808384e77a8b1e40f56a3fd2a (BERT-base-uncased, batch size 8)\n",
        "- **Model Performance**: 92.50% F1-Score, 92.33% Accuracy\n",
        "- **Target Platform**: Hugging Face Hub for public model sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlflow in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (3.4.0)\n",
            "Requirement already satisfied: huggingface_hub in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (0.35.0)\n",
            "Requirement already satisfied: transformers in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (2.8.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.4.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (3.4.0)\n",
            "Requirement already satisfied: mlflow-tracing==3.4.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (3.4.0)\n",
            "Requirement already satisfied: Flask<4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (1.16.5)\n",
            "Requirement already satisfied: cryptography<46,>=43.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (45.0.7)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: fastmcp<3,>=2.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (2.12.3)\n",
            "Requirement already satisfied: graphene<4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: matplotlib<4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (3.10.5)\n",
            "Requirement already satisfied: numpy<3 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (2.3.3)\n",
            "Requirement already satisfied: pandas<3 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (2.3.2)\n",
            "Requirement already satisfied: pyarrow<22,>=4.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (21.0.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (1.7.2)\n",
            "Requirement already satisfied: scipy<2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (1.16.2)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow) (2.0.43)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.65.0)\n",
            "Requirement already satisfied: fastapi<1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.116.2)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (6.32.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (2.11.7)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (1.1.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (2.32.5)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.14 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from cryptography<46,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (2.40.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.4.0->mlflow) (0.48.0)\n",
            "Requirement already satisfied: authlib>=1.5.2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (1.6.4)\n",
            "Requirement already satisfied: cyclopts>=3.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (3.24.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.2.2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (0.28.1)\n",
            "Requirement already satisfied: mcp<2.0.0,>=1.12.4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (1.14.1)\n",
            "Requirement already satisfied: openapi-core>=0.19.5 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (0.19.5)\n",
            "Requirement already satisfied: openapi-pydantic>=0.5.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (0.5.1)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (1.10.0)\n",
            "Requirement already satisfied: rich>=13.9.4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (14.1.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from Flask<4->mlflow) (3.1.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: zipp>=3.20 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.4.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: anyio>=4.5 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (4.10.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (3.0.2)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.4.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow) (2025.8.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny==3.4.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: filelock in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub) (2025.9.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from transformers) (2025.7.34)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: pycparser in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from cffi>=1.14->cryptography<46,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: attrs>=23.1.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (0.17.0)\n",
            "Requirement already satisfied: rich-rst<2.0.0,>=1.3.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (1.3.1)\n",
            "Requirement already satisfied: docutils in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (0.22.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (1.0.9)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.27.1)\n",
            "Requirement already satisfied: isodate in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.7.2)\n",
            "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.3.4)\n",
            "Requirement already satisfied: more-itertools in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (10.8.0)\n",
            "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.6.3)\n",
            "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.7.2)\n",
            "Requirement already satisfied: parse in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (1.20.2)\n",
            "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.4.4)\n",
            "Requirement already satisfied: rfc3339-validator in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.1.4)\n",
            "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (1.12.0)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow) (2.8.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install mlflow huggingface_hub transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jam/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n",
            "PyTorch version: 2.8.0\n",
            "MLflow version: 3.4.0\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch\n",
        "from transformers import (\n",
        "    BertForSequenceClassification, \n",
        "    BertTokenizer, \n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from huggingface_hub import HfApi, Repository, create_repo\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"MLflow version: {mlflow.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MLflow Configuration and Model Download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Databricks MLflow\n",
            "Target MLflow Run ID: ff204ab808384e77a8b1e40f56a3fd2a\n"
          ]
        }
      ],
      "source": [
        "# MLflow configuration\n",
        "# Set up MLflow tracking URI (adjust based on your setup)\n",
        "try:\n",
        "    # Try Databricks first\n",
        "    mlflow.set_tracking_uri(\"databricks\")\n",
        "    print(\"Connected to Databricks MLflow\")\n",
        "except:\n",
        "    # Fallback to local\n",
        "    mlflow.set_tracking_uri(\"file:./mlruns\")\n",
        "    print(\"Using local MLflow tracking\")\n",
        "\n",
        "# Define the run ID for the best model\n",
        "BEST_RUN_ID = \"ff204ab808384e77a8b1e40f56a3fd2a\"\n",
        "print(f\"Target MLflow Run ID: {BEST_RUN_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model from: runs:/ff204ab808384e77a8b1e40f56a3fd2a/model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading artifacts:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "Downloading artifacts:  83%|████████▎ | 5/6 [02:36<00:31, 31.38s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Download the best model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m model_path = \u001b[43mdownload_mlflow_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBEST_RUN_ID\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mdownload_mlflow_model\u001b[39m\u001b[34m(run_id, download_path)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading model from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Download model artifacts\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpytorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel downloaded successfully to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m download_path\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages/mlflow/pytorch/__init__.py:646\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_uri, dst_path, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    595\u001b[39m \u001b[33;03mLoad a PyTorch model from a local file or a run.\u001b[39;00m\n\u001b[32m    596\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    642\u001b[39m \u001b[33;03m    predict X: 30.0, y_pred: 60.48\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m local_model_path = \u001b[43m_download_artifact_from_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m pytorch_conf = _get_flavor_configuration(model_path=local_model_path, flavor_name=FLAVOR_NAME)\n\u001b[32m    648\u001b[39m _add_code_from_conf_to_system_path(local_model_path, pytorch_conf)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages/mlflow/tracking/artifact_utils.py:124\u001b[39m, in \u001b[36m_download_artifact_from_uri\u001b[39m\u001b[34m(artifact_uri, output_path, lineage_header_info, tracking_uri)\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repo, ModelsArtifactRepository):\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m repo.download_artifacts(\n\u001b[32m    120\u001b[39m             artifact_path=artifact_path,\n\u001b[32m    121\u001b[39m             dst_path=output_path,\n\u001b[32m    122\u001b[39m             lineage_header_info=lineage_header_info,\n\u001b[32m    123\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artifact_uri.startswith(\u001b[33m\"\u001b[39m\u001b[33mm-\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    127\u001b[39m         \u001b[38;5;66;03m# When a Model ID like string is passed, suggest using 'models:/{artifact_uri}' instead.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py:218\u001b[39m, in \u001b[36mRunsArtifactRepository.download_artifacts\u001b[39m\u001b[34m(self, artifact_path, dst_path)\u001b[39m\n\u001b[32m    216\u001b[39m model_out_path: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     model_out_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_model_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    220\u001b[39m     _logger.debug(\n\u001b[32m    221\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to download model artifacts from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.artifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m         exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    223\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py:247\u001b[39m, in \u001b[36mRunsArtifactRepository._download_model_artifacts\u001b[39m\u001b[34m(self, artifact_path, dst_path)\u001b[39m\n\u001b[32m    245\u001b[39m     dst = os.path.join(dst_path, model_name)\n\u001b[32m    246\u001b[39m     os.makedirs(dst, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/serendip-travel/explainable-tourism-nlp/.venv/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repo.py:304\u001b[39m, in \u001b[36mArtifactRepository.download_artifacts\u001b[39m\u001b[34m(self, artifact_path, dst_path)\u001b[39m\n\u001b[32m    302\u001b[39m tracebacks = {}\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ArtifactProgressBar.files(desc=\u001b[33m\"\u001b[39m\u001b[33mDownloading artifacts\u001b[39m\u001b[33m\"\u001b[39m, total=\u001b[38;5;28mlen\u001b[39m(futures)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n\u001b[32m    246\u001b[39m     finished = waiter.finished_futures\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Download model artifacts from MLflow\n",
        "def download_mlflow_model(run_id, download_path=\"./model_artifacts\"):\n",
        "    \"\"\"Download model artifacts from MLflow run\"\"\"\n",
        "    try:\n",
        "        # Create download directory\n",
        "        os.makedirs(download_path, exist_ok=True)\n",
        "        \n",
        "        # Download the model\n",
        "        model_uri = f\"runs:/{run_id}/model\"\n",
        "        print(f\"Downloading model from: {model_uri}\")\n",
        "        \n",
        "        # Download model artifacts\n",
        "        mlflow.pytorch.load_model(model_uri, dst_path=download_path)\n",
        "        \n",
        "        print(f\"Model downloaded successfully to: {download_path}\")\n",
        "        return download_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading model: {e}\")\n",
        "        return None\n",
        "\n",
        "# Download the best model\n",
        "model_path = download_mlflow_model(BEST_RUN_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get run information and metrics\n",
        "def get_run_info(run_id):\n",
        "    \"\"\"Get detailed information about the MLflow run\"\"\"\n",
        "    try:\n",
        "        # Get run details\n",
        "        run = mlflow.get_run(run_id)\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(\"MLFLOW RUN INFORMATION\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Run ID: {run_id}\")\n",
        "        print(f\"Run Name: {run.data.tags.get('mlflow.runName', 'N/A')}\")\n",
        "        print(f\"Status: {run.info.status}\")\n",
        "        print(f\"Start Time: {run.info.start_time}\")\n",
        "        print(f\"End Time: {run.info.end_time}\")\n",
        "        \n",
        "        print(f\"\\nParameters:\")\n",
        "        for key, value in run.data.params.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "            \n",
        "        print(f\"\\nMetrics:\")\n",
        "        for key, value in run.data.metrics.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "            \n",
        "        print(f\"\\nTags:\")\n",
        "        for key, value in run.data.tags.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "            \n",
        "        return run\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error getting run info: {e}\")\n",
        "        return None\n",
        "\n",
        "# Get run information\n",
        "run_info = get_run_info(BEST_RUN_ID)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Preparation for Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the downloaded model and prepare for Hugging Face\n",
        "def prepare_model_for_hf(model_path, hf_model_path=\"./hf_model\"):\n",
        "    \"\"\"Prepare the MLflow model for Hugging Face Hub\"\"\"\n",
        "    try:\n",
        "        # Create Hugging Face model directory\n",
        "        os.makedirs(hf_model_path, exist_ok=True)\n",
        "        \n",
        "        # Load the model from MLflow\n",
        "        print(\"Loading model from MLflow...\")\n",
        "        model = mlflow.pytorch.load_model(f\"runs:/{BEST_RUN_ID}/model\")\n",
        "        \n",
        "        # Load the tokenizer\n",
        "        print(\"Loading tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        \n",
        "        # Save model and tokenizer in Hugging Face format\n",
        "        print(f\"Saving model to {hf_model_path}...\")\n",
        "        model.save_pretrained(hf_model_path)\n",
        "        tokenizer.save_pretrained(hf_model_path)\n",
        "        \n",
        "        print(\"Model and tokenizer saved successfully!\")\n",
        "        return hf_model_path, model, tokenizer\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error preparing model: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Prepare the model\n",
        "hf_path, model, tokenizer = prepare_model_for_hf(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model configuration and metadata\n",
        "def create_model_config(hf_path, run_info):\n",
        "    \"\"\"Create configuration files for Hugging Face model\"\"\"\n",
        "    try:\n",
        "        # Model configuration\n",
        "        config = {\n",
        "            \"model_type\": \"bert\",\n",
        "            \"architectures\": [\"BertForSequenceClassification\"],\n",
        "            \"num_labels\": 4,\n",
        "            \"id2label\": {\n",
        "                \"0\": \"Regenerative & Eco-Tourism\",\n",
        "                \"1\": \"Integrated Wellness\", \n",
        "                \"2\": \"Immersive Culinary\",\n",
        "                \"3\": \"Off-the-Beaten-Path Adventure\"\n",
        "            },\n",
        "            \"label2id\": {\n",
        "                \"Regenerative & Eco-Tourism\": 0,\n",
        "                \"Integrated Wellness\": 1,\n",
        "                \"Immersive Culinary\": 2,\n",
        "                \"Off-the-Beaten-Path Adventure\": 3\n",
        "            },\n",
        "            \"problem_type\": \"multi_label_classification\"\n",
        "        }\n",
        "        \n",
        "        # Save config.json\n",
        "        with open(f\"{hf_path}/config.json\", \"w\") as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "            \n",
        "        # Create README.md\n",
        "        readme_content = f\"\"\"---\n",
        "language: en\n",
        "license: mit\n",
        "tags:\n",
        "- text-classification\n",
        "- multi-label\n",
        "- tourism\n",
        "- sri-lanka\n",
        "- bert\n",
        "- pytorch\n",
        "datasets:\n",
        "- tourism-reviews-sri-lanka\n",
        "metrics:\n",
        "- f1\n",
        "- accuracy\n",
        "- precision\n",
        "- recall\n",
        "model-index:\n",
        "- name: serendip-travel-experiential-classifier\n",
        "  results:\n",
        "  - task:\n",
        "      type: text-classification\n",
        "      name: Multi-Label Text Classification\n",
        "    dataset:\n",
        "      type: tourism-reviews-sri-lanka\n",
        "      name: Sri Lankan Tourism Reviews\n",
        "    metrics:\n",
        "    - type: f1\n",
        "      value: 0.9250\n",
        "    - type: accuracy\n",
        "      value: 0.9233\n",
        "    - type: precision\n",
        "      value: 0.9681\n",
        "    - type: recall\n",
        "      value: 0.8859\n",
        "---\n",
        "\n",
        "# Serendip Travel Experiential Classifier\n",
        "\n",
        "A fine-tuned BERT model for classifying Sri Lankan tourist reviews into four experiential dimensions.\n",
        "\n",
        "## Model Description\n",
        "\n",
        "This model is a fine-tuned BERT-base-uncased model trained on Sri Lankan tourism reviews to classify text into four experiential dimensions:\n",
        "\n",
        "1. **Regenerative & Eco-Tourism**: Travel focused on positive social and environmental impact\n",
        "2. **Integrated Wellness**: Journeys combining physical and mental well-being\n",
        "3. **Immersive Culinary**: Experiences centered on authentic local cuisine\n",
        "4. **Off-the-Beaten-Path Adventure**: Exploration of less crowded natural landscapes\n",
        "\n",
        "## Performance\n",
        "\n",
        "- **F1-Score**: 92.50%\n",
        "- **Accuracy**: 92.33%\n",
        "- **Precision**: 96.81%\n",
        "- **Recall**: 88.59%\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"your-username/serendip-travel-classifier\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"your-username/serendip-travel-classifier\")\n",
        "\n",
        "# Example prediction\n",
        "text = \"The organic tea plantation tour was amazing! We learned about sustainable farming practices.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.sigmoid(outputs.logits)\n",
        "    \n",
        "# Get predicted labels\n",
        "labels = [\"Regenerative & Eco-Tourism\", \"Integrated Wellness\", \"Immersive Culinary\", \"Off-the-Beaten-Path Adventure\"]\n",
        "predicted_labels = [labels[i] for i, score in enumerate(predictions[0]) if score > 0.5]\n",
        "print(f\"Predicted labels: {predicted_labels}\")\n",
        "```\n",
        "\n",
        "## Training Details\n",
        "\n",
        "- **Base Model**: bert-base-uncased\n",
        "- **Training Data**: 16,156 Sri Lankan tourism reviews\n",
        "- **Learning Rate**: 2e-05\n",
        "- **Batch Size**: 8\n",
        "- **Epochs**: 5\n",
        "- **Framework**: PyTorch with MLflow tracking\n",
        "\n",
        "## Citation\n",
        "\n",
        "```bibtex\n",
        "@misc{{serendip-travel-classifier,\n",
        "  title={{Serendip Travel Experiential Classifier}},\n",
        "  author={{B M J N Balasuriya}},\n",
        "  year={{2025}},\n",
        "  publisher={{Hugging Face}},\n",
        "  howpublished={{\\\\url{{https://huggingface.co/your-username/serendip-travel-classifier}}}}\n",
        "}}\n",
        "```\n",
        "\"\"\"\n",
        "        \n",
        "        # Save README.md\n",
        "        with open(f\"{hf_path}/README.md\", \"w\") as f:\n",
        "            f.write(readme_content)\n",
        "            \n",
        "        print(\"Configuration files created successfully!\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating config: {e}\")\n",
        "        return False\n",
        "\n",
        "# Create configuration files\n",
        "if hf_path and run_info:\n",
        "    create_model_config(hf_path, run_info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Testing and Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the prepared model\n",
        "def test_model_prediction(model, tokenizer, test_texts):\n",
        "    \"\"\"Test the model with sample texts\"\"\"\n",
        "    try:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"MODEL TESTING\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        labels = [\n",
        "            \"Regenerative & Eco-Tourism\",\n",
        "            \"Integrated Wellness\", \n",
        "            \"Immersive Culinary\",\n",
        "            \"Off-the-Beaten-Path Adventure\"\n",
        "        ]\n",
        "        \n",
        "        for i, text in enumerate(test_texts, 1):\n",
        "            print(f\"\\nTest {i}: {text[:100]}...\")\n",
        "            \n",
        "            # Tokenize input\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "            \n",
        "            # Get predictions\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                predictions = torch.sigmoid(outputs.logits)\n",
        "            \n",
        "            # Display results\n",
        "            print(\"Predicted labels:\")\n",
        "            for j, (label, score) in enumerate(zip(labels, predictions[0])):\n",
        "                status = \"✓\" if score > 0.5 else \"✗\"\n",
        "                print(f\"  {status} {label}: {score:.3f}\")\n",
        "                \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error testing model: {e}\")\n",
        "        return False\n",
        "\n",
        "# Sample test texts\n",
        "test_texts = [\n",
        "    \"The organic tea plantation tour was amazing! We learned about sustainable farming practices and environmental conservation.\",\n",
        "    \"The spa retreat offered incredible yoga sessions and meditation classes. Perfect for relaxation and wellness.\",\n",
        "    \"The local cooking class was fantastic! We learned to make authentic Sri Lankan curry with fresh spices from the market.\",\n",
        "    \"The hiking trail through the remote jungle was challenging but rewarding. We saw amazing wildlife and untouched nature.\"\n",
        "]\n",
        "\n",
        "# Test the model\n",
        "if model and tokenizer:\n",
        "    test_model_prediction(model, tokenizer, test_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Hugging Face Hub Upload\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face Hub upload setup\n",
        "def setup_huggingface_upload():\n",
        "    \"\"\"Setup Hugging Face Hub for model upload\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"HUGGING FACE HUB SETUP\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"To upload your model to Hugging Face Hub, you need to:\")\n",
        "    print(\"1. Create a Hugging Face account at https://huggingface.co/\")\n",
        "    print(\"2. Generate an access token at https://huggingface.co/settings/tokens\")\n",
        "    print(\"3. Install and login to huggingface_hub:\")\n",
        "    print(\"   !pip install huggingface_hub\")\n",
        "    print(\"   !huggingface-cli login\")\n",
        "    print(\"4. Update the model name below with your username\")\n",
        "    \n",
        "    # Model name (update with your username)\n",
        "    model_name = \"your-username/serendip-travel-classifier\"\n",
        "    print(f\"\\nModel will be uploaded as: {model_name}\")\n",
        "    \n",
        "    return model_name\n",
        "\n",
        "# Setup instructions\n",
        "model_name = setup_huggingface_upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload model to Hugging Face Hub\n",
        "def upload_to_huggingface(hf_path, model_name):\n",
        "    \"\"\"Upload the prepared model to Hugging Face Hub\"\"\"\n",
        "    try:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"UPLOADING TO HUGGING FACE HUB\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Initialize Hugging Face API\n",
        "        api = HfApi()\n",
        "        \n",
        "        # Create repository\n",
        "        print(f\"Creating repository: {model_name}\")\n",
        "        create_repo(model_name, exist_ok=True, private=False)\n",
        "        \n",
        "        # Upload all files\n",
        "        print(f\"Uploading files from {hf_path}...\")\n",
        "        api.upload_folder(\n",
        "            folder_path=hf_path,\n",
        "            repo_id=model_name,\n",
        "            repo_type=\"model\"\n",
        "        )\n",
        "        \n",
        "        print(f\"✅ Model successfully uploaded to: https://huggingface.co/{model_name}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error uploading to Hugging Face: {e}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"1. Make sure you're logged in: huggingface-cli login\")\n",
        "        print(\"2. Check your access token permissions\")\n",
        "        print(\"3. Verify the model name is correct\")\n",
        "        return False\n",
        "\n",
        "# Upload the model (uncomment when ready)\n",
        "# upload_to_huggingface(hf_path, model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Alternative Upload Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Manual upload using git\n",
        "def create_git_upload_instructions(hf_path, model_name):\n",
        "    \"\"\"Create instructions for manual git upload\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ALTERNATIVE: MANUAL GIT UPLOAD\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"If the API upload fails, you can use git to upload manually:\")\n",
        "    print()\n",
        "    print(\"1. Clone the repository:\")\n",
        "    print(f\"   git clone https://huggingface.co/{model_name}\")\n",
        "    print()\n",
        "    print(\"2. Copy files to the repository:\")\n",
        "    print(f\"   cp -r {hf_path}/* {model_name}/\")\n",
        "    print()\n",
        "    print(\"3. Add, commit, and push:\")\n",
        "    print(\"   cd {model_name}\")\n",
        "    print(\"   git add .\")\n",
        "    print(\"   git commit -m 'Add model files'\")\n",
        "    print(\"   git push\")\n",
        "    print()\n",
        "    print(\"4. Or use the web interface:\")\n",
        "    print(f\"   https://huggingface.co/{model_name}\")\n",
        "    print(\"   - Click 'Add file' -> 'Upload files'\")\n",
        "    print(f\"   - Upload all files from {hf_path}/\")\n",
        "    \n",
        "    # List files to upload\n",
        "    print(f\"\\nFiles to upload from {hf_path}:\")\n",
        "    if os.path.exists(hf_path):\n",
        "        for file in os.listdir(hf_path):\n",
        "            print(f\"  - {file}\")\n",
        "\n",
        "# Create manual upload instructions\n",
        "create_git_upload_instructions(hf_path, model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Usage Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create usage examples for the deployed model\n",
        "def create_usage_examples():\n",
        "    \"\"\"Create comprehensive usage examples\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"MODEL USAGE EXAMPLES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    examples = \"\"\"\n",
        "# Example 1: Basic Usage\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"your-username/serendip-travel-classifier\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"your-username/serendip-travel-classifier\")\n",
        "\n",
        "# Example text\n",
        "text = \"The organic tea plantation tour was amazing! We learned about sustainable farming practices.\"\n",
        "\n",
        "# Tokenize and predict\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.sigmoid(outputs.logits)\n",
        "\n",
        "# Get predicted labels\n",
        "labels = [\"Regenerative & Eco-Tourism\", \"Integrated Wellness\", \"Immersive Culinary\", \"Off-the-Beaten-Path Adventure\"]\n",
        "predicted_labels = [labels[i] for i, score in enumerate(predictions[0]) if score > 0.5]\n",
        "print(f\"Predicted labels: {predicted_labels}\")\n",
        "\n",
        "# Example 2: Batch Processing\n",
        "texts = [\n",
        "    \"The spa retreat offered incredible yoga sessions and meditation classes.\",\n",
        "    \"The local cooking class was fantastic! We learned to make authentic Sri Lankan curry.\",\n",
        "    \"The hiking trail through the remote jungle was challenging but rewarding.\"\n",
        "]\n",
        "\n",
        "# Process multiple texts\n",
        "for text in texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.sigmoid(outputs.logits)\n",
        "    \n",
        "    predicted_labels = [labels[i] for i, score in enumerate(predictions[0]) if score > 0.5]\n",
        "    print(f\"Text: {text[:50]}...\")\n",
        "    print(f\"Labels: {predicted_labels}\")\n",
        "    print()\n",
        "\n",
        "# Example 3: Confidence Scores\n",
        "def get_confidence_scores(text, threshold=0.5):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.sigmoid(outputs.logits)\n",
        "    \n",
        "    results = []\n",
        "    for i, (label, score) in enumerate(zip(labels, predictions[0])):\n",
        "        results.append({\n",
        "            'label': label,\n",
        "            'score': float(score),\n",
        "            'predicted': score > threshold\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Get detailed confidence scores\n",
        "text = \"The organic tea plantation tour was amazing! We learned about sustainable farming practices.\"\n",
        "scores = get_confidence_scores(text)\n",
        "for result in scores:\n",
        "    print(f\"{result['label']}: {result['score']:.3f} ({'✓' if result['predicted'] else '✗'})\")\n",
        "\"\"\"\n",
        "    \n",
        "    print(examples)\n",
        "\n",
        "# Display usage examples\n",
        "create_usage_examples()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary and next steps\n",
        "def deployment_summary():\n",
        "    \"\"\"Provide deployment summary and next steps\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"DEPLOYMENT SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"✅ Model Preparation Complete!\")\n",
        "    print(f\"📁 Model files saved to: {hf_path}\")\n",
        "    print(f\"🏷️  Model name: {model_name}\")\n",
        "    print(f\"📊 Performance: 92.50% F1-Score, 92.33% Accuracy\")\n",
        "    \n",
        "    print(\"\\n📋 Next Steps:\")\n",
        "    print(\"1. 🔐 Login to Hugging Face: huggingface-cli login\")\n",
        "    print(\"2. ✏️  Update model name with your username\")\n",
        "    print(\"3. 🚀 Uncomment and run the upload function\")\n",
        "    print(\"4. 📝 Update README.md with your actual model URL\")\n",
        "    print(\"5. 🧪 Test the deployed model\")\n",
        "    print(\"6. 📢 Share your model with the community!\")\n",
        "    \n",
        "    print(\"\\n📁 Files Created:\")\n",
        "    if hf_path and os.path.exists(hf_path):\n",
        "        for file in os.listdir(hf_path):\n",
        "            print(f\"  - {file}\")\n",
        "    \n",
        "    print(f\"\\n🔗 Model will be available at:\")\n",
        "    print(f\"   https://huggingface.co/{model_name}\")\n",
        "    \n",
        "    print(\"\\n🎯 Model Features:\")\n",
        "    print(\"  - Multi-label text classification\")\n",
        "    print(\"  - 4 experiential dimensions\")\n",
        "    print(\"  - BERT-base-uncased architecture\")\n",
        "    print(\"  - 92.50% F1-Score performance\")\n",
        "    print(\"  - Ready for production use\")\n",
        "    \n",
        "    print(\"\\n💡 Usage Tips:\")\n",
        "    print(\"  - Use threshold 0.5 for binary predictions\")\n",
        "    print(\"  - Adjust threshold based on use case\")\n",
        "    print(\"  - Consider confidence scores for uncertainty\")\n",
        "    print(\"  - Batch processing for efficiency\")\n",
        "\n",
        "# Display summary\n",
        "deployment_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.4)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
