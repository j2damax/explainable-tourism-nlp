{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c33ff14",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "This notebook covers the training and evaluation of transformer-based multi-label text classification models for Sri Lankan tourist review analysis.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Multi-label classification of tourist reviews into experiential dimensions\n",
    "- **Models**: BERT and RoBERTa transformer models\n",
    "- **Labels**: 4 experiential dimensions (Regenerative & Eco-Tourism, Integrated Wellness, Immersive Culinary, Off-the-Beaten-Path Adventure)\n",
    "- **Framework**: PyTorch with MLflow experiment tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b750c78",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import (\n",
    "    BertForSequenceClassification, \n",
    "    RobertaForSequenceClassification, \n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, multilabel_confusion_matrix, classification_report\n",
    ")\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.models import infer_signature\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Google Colab integration (if running in Colab)\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    drive.mount('/content/drive')\n",
    "    DATABRICKS_HOST = userdata.get(\"DATABRICKS_HOST\")\n",
    "    DATABRICKS_TOKEN = userdata.get(\"DATABRICKS_TOKEN\")\n",
    "    print(\"Google Colab environment detected\")\n",
    "except ImportError:\n",
    "    # Local environment\n",
    "    DATABRICKS_HOST = os.getenv(\"DATABRICKS_HOST\")\n",
    "    DATABRICKS_TOKEN = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "    print(\"Local environment detected\")\n",
    "\n",
    "# MLflow setup\n",
    "if DATABRICKS_HOST and DATABRICKS_TOKEN:\n",
    "    os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "    os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    mlflow.set_experiment(\"/Users/j2damax@gmail.com/serendip-travel-review-classifier-experiments\")\n",
    "    print(\"MLflow configured with Databricks\")\n",
    "else:\n",
    "    mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "    mlflow.set_experiment(\"serendip-travel-experiments\")\n",
    "    print(\"MLflow configured with local tracking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train_data = torch.load('../data/processed/train_dataset.pt', weights_only=False)\n",
    "    test_data = torch.load('../data/processed/test_dataset.pt', weights_only=False)\n",
    "    \n",
    "    train_dataset = ReviewsDataset(train_data['encodings'], train_data['labels'])\n",
    "    test_dataset = ReviewsDataset(test_data['encodings'], test_data['labels'])\n",
    "    \n",
    "    num_labels = train_data['labels'].shape[1]\n",
    "    print(f\"Dataset loaded successfully:\")\n",
    "    print(f\"  Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Test samples: {len(test_dataset)}\")\n",
    "    print(f\"  Number of labels: {num_labels}\")\n",
    "    print(f\"  Label shape: {train_data['labels'].shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Dataset files not found. {e}\")\n",
    "    print(\"Please ensure the processed datasets are available in '../data/processed/'\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdc5bc",
   "metadata": {},
   "source": [
    "## 2. Model Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae87add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    epochs=3,\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    train_dataset=None,\n",
    "    test_dataset=None,\n",
    "    num_labels=None,\n",
    "    experiment_name=\"model-experiment\",\n",
    "    validation_split=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate a transformer model for multi-label classification.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training history and final predictions\n",
    "    \"\"\"\n",
    "    # Split training data for validation\n",
    "    train_size = int((1 - validation_split) * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Model selection\n",
    "    if model_name.startswith(\"roberta\"):\n",
    "        model_cls = RobertaForSequenceClassification\n",
    "    else:\n",
    "        model_cls = BertForSequenceClassification\n",
    "    \n",
    "    model = model_cls.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        problem_type='multi_label_classification'\n",
    "    )\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(run_name=experiment_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            all_val_preds = []\n",
    "            all_val_labels = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    val_loss = outputs.loss\n",
    "                    total_val_loss += val_loss.item()\n",
    "                    \n",
    "                    logits = outputs.logits.cpu().numpy()\n",
    "                    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "                    all_val_preds.append(preds)\n",
    "                    all_val_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            all_val_preds = np.vstack(all_val_preds)\n",
    "            all_val_labels = np.vstack(all_val_labels)\n",
    "            \n",
    "            val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "            val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "            \n",
    "            # Store history\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            history['val_f1'].append(val_f1)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "            mlflow.log_metric(\"val_f1\", val_f1, step=epoch)\n",
    "        \n",
    "        # Final test evaluation\n",
    "        model.eval()\n",
    "        all_test_preds = []\n",
    "        all_test_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].cpu().numpy()\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits.cpu().numpy()\n",
    "                preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "                all_test_preds.append(preds)\n",
    "                all_test_labels.append(labels)\n",
    "        \n",
    "        all_test_preds = np.vstack(all_test_preds)\n",
    "        all_test_labels = np.vstack(all_test_labels)\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "        test_f1 = f1_score(all_test_labels, all_test_preds, average='macro')\n",
    "        test_precision = precision_score(all_test_labels, all_test_preds, average='macro', zero_division=0)\n",
    "        test_recall = recall_score(all_test_labels, all_test_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        # Calculate per-label metrics\n",
    "        f1_scores = [f1_score(all_test_labels[:, i], all_test_preds[:, i], zero_division=0) for i in range(num_labels)]\n",
    "        precision_scores = [precision_score(all_test_labels[:, i], all_test_preds[:, i], zero_division=0) for i in range(num_labels)]\n",
    "        recall_scores = [recall_score(all_test_labels[:, i], all_test_preds[:, i], zero_division=0) for i in range(num_labels)]\n",
    "        \n",
    "        print(f\"\\nFinal Test Results:\")\n",
    "        print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"F1-Score: {test_f1:.4f}\")\n",
    "        print(f\"Precision: {test_precision:.4f}\")\n",
    "        print(f\"Recall: {test_recall:.4f}\")\n",
    "        \n",
    "        # Log final metrics\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_f1\", test_f1)\n",
    "        mlflow.log_metric(\"test_precision\", test_precision)\n",
    "        mlflow.log_metric(\"test_recall\", test_recall)\n",
    "        \n",
    "        # Log per-label metrics\n",
    "        for i, (f1, p, r) in enumerate(zip(f1_scores, precision_scores, recall_scores)):\n",
    "            mlflow.log_metric(f\"f1_label_{i}\", f1)\n",
    "            mlflow.log_metric(f\"precision_label_{i}\", p)\n",
    "            mlflow.log_metric(f\"recall_label_{i}\", r)\n",
    "        \n",
    "        # Generate and save classification report\n",
    "        report = classification_report(all_test_labels, all_test_preds, output_dict=True, zero_division=0)\n",
    "        with open(\"classification_report.json\", \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        mlflow.log_artifact(\"classification_report.json\")\n",
    "        \n",
    "        # Save model\n",
    "        example_batch = next(iter(test_loader))\n",
    "        inputs = {\n",
    "            \"input_ids\": example_batch[\"input_ids\"][:1].cpu().numpy(),\n",
    "            \"attention_mask\": example_batch[\"attention_mask\"][:1].cpu().numpy()\n",
    "        }\n",
    "        outputs = model(\n",
    "            input_ids=example_batch[\"input_ids\"][:1].to(device),\n",
    "            attention_mask=example_batch[\"attention_mask\"][:1].to(device)\n",
    "        ).logits.cpu().detach().numpy()\n",
    "        \n",
    "        signature = infer_signature(inputs, outputs)\n",
    "        mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "        \n",
    "        # Save predictions\n",
    "        np.save(\"test_predictions.npy\", all_test_preds)\n",
    "        mlflow.log_artifact(\"test_predictions.npy\")\n",
    "    \n",
    "    return {\n",
    "        'history': history,\n",
    "        'test_predictions': all_test_preds,\n",
    "        'test_labels': all_test_labels,\n",
    "        'model': model,\n",
    "        'metrics': {\n",
    "            'accuracy': test_accuracy,\n",
    "            'f1': test_f1,\n",
    "            'precision': test_precision,\n",
    "            'recall': test_recall\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b82bcf",
   "metadata": {},
   "source": [
    "## 3. Model Training Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d22e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label names\n",
    "label_names = [\n",
    "    'Regenerative & Eco-Tourism',\n",
    "    'Integrated Wellness',\n",
    "    'Immersive Culinary',\n",
    "    'Off-the-Beaten-Path Adventure'\n",
    "]\n",
    "\n",
    "# Training configurations\n",
    "configs = [\n",
    "    {'model': 'bert-base-uncased', 'lr': 2e-5, 'bs': 8},\n",
    "    {'model': 'bert-base-uncased', 'lr': 2e-5, 'bs': 16},\n",
    "    {'model': 'roberta-base', 'lr': 2e-5, 'bs': 8},\n",
    "    {'model': 'roberta-base', 'lr': 2e-5, 'bs': 16}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"Training {config['model']} with lr={config['lr']}, bs={config['bs']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = train_and_evaluate_model(\n",
    "        model_name=config['model'],\n",
    "        epochs=3,\n",
    "        batch_size=config['bs'],\n",
    "        learning_rate=config['lr'],\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        num_labels=num_labels,\n",
    "        experiment_name=f\"{config['model']}-lr{config['lr']}-bs{config['bs']}\"\n",
    "    )\n",
    "    \n",
    "    result['config'] = config\n",
    "    results.append(result)\n",
    "\n",
    "print(\"All experiments completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252923a",
   "metadata": {},
   "source": [
    "## 4. Results Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_result = max(results, key=lambda x: x['metrics']['f1'])\n",
    "print(f\"Best model: {best_result['config']['model']} with F1-Score: {best_result['metrics']['f1']:.4f}\")\n",
    "\n",
    "# Create results summary\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': r['config']['model'],\n",
    "        'Learning Rate': r['config']['lr'],\n",
    "        'Batch Size': r['config']['bs'],\n",
    "        'Test Accuracy': r['metrics']['accuracy'],\n",
    "        'Test F1': r['metrics']['f1'],\n",
    "        'Test Precision': r['metrics']['precision'],\n",
    "        'Test Recall': r['metrics']['recall']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "print(\"Results Summary:\")\n",
    "print(results_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training loss plotting function\n",
    "def plot_training_loss_from_mlflow(model_type):\n",
    "    \"\"\"Plot training loss over epochs for a specific model type from MLflow runs.\"\"\"\n",
    "    try:\n",
    "        # Get experiment ID\n",
    "        experiment = mlflow.get_experiment_by_name(\"/Users/j2damax@gmail.com/serendip-travel-review-classifier-experiments\")\n",
    "        if not experiment:\n",
    "            experiment = mlflow.get_experiment_by_name(\"serendip-travel-experiments\")\n",
    "        \n",
    "        # Fetch all runs from our experiment\n",
    "        runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "        runs_with_loss = runs[runs['tags.mlflow.runName'].notna()]\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Group by run name and filter by model type\n",
    "        for run_name, run_df in runs_with_loss.groupby('tags.mlflow.runName'):\n",
    "            if run_name.startswith(model_type):\n",
    "                # Extract metrics for this run\n",
    "                run_id = run_df['run_id'].iloc[0]\n",
    "                try:\n",
    "                    run_metrics = mlflow.tracking.MlflowClient().get_metric_history(run_id, 'train_loss')\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not retrieve metrics for run {run_id}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Extract epochs and loss values\n",
    "                epochs = [metric.step for metric in run_metrics]\n",
    "                loss = [metric.value for metric in run_metrics]\n",
    "\n",
    "                if not epochs:\n",
    "                    print(f\"No training loss metrics found for run {run_id}\")\n",
    "                    continue\n",
    "\n",
    "                # Plot this run's training loss\n",
    "                linestyle = '-' if model_type == 'bert' else '--'\n",
    "                plt.plot(epochs, loss, marker='o', linestyle=linestyle, label=run_name)\n",
    "\n",
    "        plt.title(f'Training Loss Over Epochs for {model_type.capitalize()} Models', fontsize=14)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Training Loss', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot MLflow training loss: {e}\")\n",
    "        print(\"Falling back to local training curves...\")\n",
    "\n",
    "# Plot training curves for best model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(f'Training Curves - {best_result[\"config\"][\"model\"]}', fontsize=14)\n",
    "\n",
    "history = best_result['history']\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Training and validation loss\n",
    "axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Validation accuracy\n",
    "axes[0, 1].plot(epochs, history['val_accuracy'], 'g-', label='Validation Accuracy')\n",
    "axes[0, 1].set_title('Validation Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Validation F1\n",
    "axes[1, 0].plot(epochs, history['val_f1'], 'm-', label='Validation F1')\n",
    "axes[1, 0].set_title('Validation F1-Score')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('F1-Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Model comparison\n",
    "model_names = [r['config']['model'] for r in results]\n",
    "f1_scores = [r['metrics']['f1'] for r in results]\n",
    "axes[1, 1].bar(model_names, f1_scores, color=['blue', 'blue', 'orange', 'orange'])\n",
    "axes[1, 1].set_title('Model Comparison (F1-Score)')\n",
    "axes[1, 1].set_ylabel('F1-Score')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot MLflow training loss if available\n",
    "print(\"\\nPlotting training loss from MLflow runs...\")\n",
    "plot_training_loss_from_mlflow('bert')\n",
    "plot_training_loss_from_mlflow('roberta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for best model\n",
    "y_true = best_result['test_labels']\n",
    "y_pred = best_result['test_predictions']\n",
    "\n",
    "# Per-label confusion matrices\n",
    "cm_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Per-Label Confusion Matrices', fontsize=14)\n",
    "\n",
    "for i, (cm, label_name) in enumerate(zip(cm_matrices, label_names)):\n",
    "    row, col = i // 2, i % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "    \n",
    "    ax.set_title(label_name, fontsize=12)\n",
    "    \n",
    "    # Add metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    ax.text(0.02, 0.98, f'F1: {f1:.3f}\\\\nP: {precision:.3f}\\\\nR: {recall:.3f}', \n",
    "            transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec213aea",
   "metadata": {},
   "source": [
    "## 5. Validation and Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebddeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation and summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING AND EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Experiment Results:\")\n",
    "print(f\"  Total experiments run: {len(results)}\")\n",
    "print(f\"  Best performing model: {best_result['config']['model']}\")\n",
    "print(f\"  Best F1-Score: {best_result['metrics']['f1']:.4f}\")\n",
    "print(f\"  Best Accuracy: {best_result['metrics']['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output Files Generated:\")\n",
    "print(f\"  - training_results.json: Complete experiment results\")\n",
    "print(f\"  - test_predictions.npy: Best model predictions\")\n",
    "print(f\"  - classification_report.json: Detailed performance metrics\")\n",
    "\n",
    "print(f\"\\nüî¨ MLflow Integration:\")\n",
    "print(f\"  - All experiments logged to MLflow\")\n",
    "print(f\"  - Models saved with signatures\")\n",
    "print(f\"  - Artifacts and metrics tracked\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training and evaluation completed successfully!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_summary = {\n",
    "    'best_model': {\n",
    "        'config': best_result['config'],\n",
    "        'metrics': best_result['metrics']\n",
    "    },\n",
    "    'all_results': [\n",
    "        {\n",
    "            'config': r['config'],\n",
    "            'metrics': r['metrics']\n",
    "        }\n",
    "        for r in results\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('training_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"Results saved to training_results.json\")\n",
    "print(\"Training and evaluation completed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74ac7f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
